{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"naive_bayes.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1LEM098IUskCNssH6ZRmxMJJStG7M2Kpw","authorship_tag":"ABX9TyMbUcpSde+4IYSSe+D9djEV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zousk-Uz-BZw"},"source":["import numpy as np\n","import pandas as pd\n","import string\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXVRp9kdLYtJ"},"source":["spam=[] \n","corpus=[]\n","train=[]\n","test=[]\n","train_spam=[]\n","test_spam=[]\n","\n","corpus_dict=set()\n","inv_index={}\n","\n","prob_spam=[]\n","prob_ham=[]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgHz6Reb46Fl"},"source":["def initialize():\n","    global test,train,train_spam,test_spam,corpus_dict,inv_index,prob_spam,prob_ham\n","    train=[]\n","    test=[]\n","    train_spam=[]\n","    test_spam=[]\n","    corpus_dict=set()\n","    inv_index={}\n","    prob_spam=[]\n","    prob_ham=[]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Es_Rg6CTD25R"},"source":["#remove stopwords and punctuations\n","def process_text(text):\n","  nopunc=[char for char in text if char not in string.punctuation]\n","  nopunc=''.join(nopunc)\n","  stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n","  clean_words=[word.lower() for word in nopunc.split() if word.lower() not in stop_words]\n","  #print(clean_words)\n","  return clean_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTi7ZpKKlUzH"},"source":["def k_fold(k):\n","    global corpus,spam,train,test,train_spam,test_spam\n","    \n","    #update train and test spam\n","    for s in spam[k]:\n","        test_spam.append(s)\n","    for j in range(7):\n","        if k!=j:\n","            for s in spam[j]:\n","                train_spam.append(s)\n","    #update train and test\n","    for sentence in corpus[k]:\n","        test.append(sentence)\n","    for j in range(7):\n","        if k!=j:\n","            for sentence in corpus[j]:\n","                train.append(sentence)\n","    #print(corpus)\n","    #print(spam)\n","    #print(train)\n","    #print(test)\n","    #print(train_spam)\n","    #print(test_spam)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ui_y-Q0BwL5z"},"source":["def make_dict():\n","    global corpus_dict,train\n","    corpus_dict=set()\n","    for sentence in train:\n","        for word in sentence:\n","            corpus_dict.add(word)\n","    #print(corpus_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQEKN7rsf5uQ"},"source":["def build_inv_index():\n","    global corpus_dict,train,inv_index\n","    inv_index = {i:[] for i in corpus_dict}\n","    #print(type(inv_index))\n","    for idx, text in enumerate(train):\n","        for word in text:\n","            inv_index[word].append(idx)\n","    #print(inv_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMEqKLll3IcP"},"source":["def corpus_prob():\n","    global prob_spam,prob_ham,inv_index,train_spam,corpus_dict\n","    total_spam=sum(train_spam)\n","    total_ham=len(train_spam)-total_spam\n","    for word in corpus_dict:\n","        no_of_spam=0\n","        no_of_ham=0\n","        #print(inv_index[word])\n","        for idx in inv_index[word]:\n","            #print(idx)\n","            #print(len(train_spam))\n","            if train_spam[idx]==1:\n","                no_of_spam+=1\n","            else:\n","                no_of_ham+=1\n","        curr_spam_prob=no_of_spam/total_spam\n","        curr_ham_prob=no_of_ham/total_ham\n","        prob_spam.append(curr_spam_prob)\n","        prob_ham.append(curr_ham_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFcSxx4Zcu9i"},"source":["def find_prob(clean_words,total_spam_prob,total_ham_prob):\n","    global prob_spam,prob_ham,train_spam\n","    no_of_spam=sum(train_spam)\n","    no_of_ham=len(train_spam)-no_of_spam\n","    spam_test=no_of_spam/(no_of_spam+no_of_ham)\n","    ham_test=no_of_spam/(no_of_spam+no_of_ham)\n","    for word in clean_words:\n","        curr_spam=0\n","        curr_ham=0\n","        if word in inv_index:\n","            for id in inv_index[word]:\n","                if train_spam[id]==1:\n","                    curr_spam+=1\n","                else:\n","                    curr_ham+=1\n","        prob_spam_curr=(curr_spam+1)/(no_of_spam+2)\n","        prob_ham_curr=(curr_ham+1)/(no_of_ham+2)\n","        spam_test*=prob_spam_curr\n","        ham_test*=prob_ham_curr\n","    if spam_test>ham_test:\n","        return 1\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lV-Yp1Qc8sR3"},"source":["def test_prob():\n","    global test,train_spam,test_spam\n","    total_spam_prob=sum(train_spam)/len(train_spam)\n","    total_ham_prob=1-total_spam_prob\n","    correct=0\n","    total=0\n","    for id,sentence in enumerate(test):\n","        is_spam=find_prob(sentence,total_spam_prob,total_ham_prob)\n","        if is_spam==test_spam[id]:\n","            correct+=1\n","        total+=1\n","    return correct/total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxYhMPP1-50I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616302944751,"user_tz":-330,"elapsed":4276,"user":{"displayName":"Nithya Thokala","photoUrl":"https://lh4.googleusercontent.com/-ujtx8oycv70/AAAAAAAAAAI/AAAAAAAAANM/P3dELJTO8NI/s64/photo.jpg","userId":"02016025551301560069"}},"outputId":"76592b16-aba6-44fa-95ad-9672eb03379f"},"source":["#main\n","\n","f=open(\"/content/drive/MyDrive/Naive bayes/dataset_NB.txt\",\"r\")\n","\n","for line in f:\n","    words=line.split()\n","    words_without_num=words[:-1]  #removing the spam value in the line\n","    line_formed=' '.join([str(elem) for elem in words_without_num]) #joining the rest of the words for line\n","\n","    clean_words=process_text(line_formed) #remove stopwords and 1-2 letter words\n","    s=set(clean_words)\n","    corpus.append(s)  \n","\n","    is_spam=words[-1] \n","    spam.append(int(is_spam)) #add the spam value to the global spam vector\n","\n","#shuffle\n","temp=list(zip(corpus,spam))\n","random.seed(0)\n","random.shuffle(temp)\n","corpus1,spam1=zip(*temp)\n","\n","#split in k-fold\n","split_list=[143,286,429,572,715,858]\n","corpus_mod=[corpus1[i:j] for i,j in zip([0] + split_list, split_list + [None])]\n","spam_mod=[spam1[i:j] for i,j in zip([0] + split_list, split_list + [None])]\n","corpus=corpus_mod\n","spam=spam_mod\n","\n","sum_of_eff=0\n","for i in range(7):\n","    initialize()\n","    k_fold(i)\n","    make_dict()\n","    build_inv_index()\n","    corpus_prob()\n","    eff=test_prob()\n","    sum_of_eff+=eff\n","    print(\"Accuracy at\",i,\"th fold is\",eff)\n","print(\"Overall accuracy: \",sum_of_eff/7)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy at 0 th fold is 0.7832167832167832\n","Accuracy at 1 th fold is 0.8181818181818182\n","Accuracy at 2 th fold is 0.8391608391608392\n","Accuracy at 3 th fold is 0.7832167832167832\n","Accuracy at 4 th fold is 0.7762237762237763\n","Accuracy at 5 th fold is 0.7692307692307693\n","Accuracy at 6 th fold is 0.8309859154929577\n","Overall accuracy:  0.8000309549605324\n"],"name":"stdout"}]}]}